{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported preprocess in 61.20476937294006 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/project/Stat/1155223034/miniconda/envs/atacformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported databank in 499.0348093509674 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('/lustre/project/Stat/1155223034/atacFormer/data')\n",
    "\n",
    "start_time = time.time()\n",
    "import preprocess\n",
    "print(f\"Imported preprocess in {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "import databank\n",
    "print(f\"Imported databank in {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/lustre/project/Stat/1155223034/atacFormer/data/heart\")\n",
    "adata_atac = sc.read_h5ad(data_dir / \"HBM233.GKRM.627_ATAC.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Structure of the sc-ATAC （Ann_data)\n",
    "\n",
    "atac.X: cell-by-peak matrix whose entries represents is there a peak in the cell. i.e X_(i,j) = True if there is a peak j in cell i.\n",
    "\n",
    "The property of rows are stored in the atac.obs (here the rows represent cells)\n",
    "\n",
    "The property of columns are stored in the atac.obsm (here the columns represent peaks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 24982 × 536282\n",
      "    obs: 'cluster'\n",
      "    obsm: 'X_umap'\n"
     ]
    }
   ],
   "source": [
    "print(adata_atac) # cell by peak matrix, each colomn denotes a peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(adata_atac.X[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1:10001-15000', '1:15001-20000', '1:50001-55000', '1:55001-60000',\n",
      "       '1:60001-65000', '1:65001-70000', '1:70001-75000', '1:75001-80000',\n",
      "       '1:80001-85000', '1:85001-90000',\n",
      "       ...\n",
      "       'Un_JTFH01001780v1_decoy:1-5000', 'Un_JTFH01001781v1_decoy:1-5000',\n",
      "       'Un_JTFH01001782v1_decoy:1-5000', 'Un_JTFH01001784v1_decoy:1-5000',\n",
      "       'Un_JTFH01001785v1_decoy:1-5000', 'Un_JTFH01001786v1_decoy:1-5000',\n",
      "       'HLA-DQA1*04:02:1-5000', 'HLA-DRB1*08:03:02:1-5000',\n",
      "       'HLA-DRB1*08:03:02:5001-10000', 'HLA-DRB1*11:01:02:5001-10000'],\n",
      "      dtype='object', length=536282)\n"
     ]
    }
   ],
   "source": [
    "print(adata_atac.var_names) # peak names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bins_file = \"/lustre/project/Stat/1155223034/atacFormer/data/bins_5k_table_23chr.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 530171\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n"
     ]
    }
   ],
   "source": [
    "adata_atac = preprocess.preprocessor(adata_atac, filter_bin=True,bin_file=filtered_bins_file, filter_cell_by_bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering bins ...\n",
      "606199\n",
      "not intersect\n",
      "Current batch size: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:42<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n"
     ]
    }
   ],
   "source": [
    "bdata = preprocess.preprocessor(adata_atac, filter_bin=True,intersect =False, bin_file=filtered_bins_file, filter_cell_by_bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1:1-5000', '1:5001-10000', '1:10001-15000', '1:15001-20000',\n",
      "       '1:20001-25000', '1:25001-30000', '1:30001-35000', '1:35001-40000',\n",
      "       '1:40001-45000', '1:45001-50000',\n",
      "       ...\n",
      "       'X:155990001-155995000', 'X:155995001-156000000',\n",
      "       'X:156000001-156005000', 'X:156005001-156010000',\n",
      "       'X:156010001-156015000', 'X:156015001-156020000',\n",
      "       'X:156020001-156025000', 'X:156025001-156030000',\n",
      "       'X:156030001-156035000', 'X:156035001-156040000'],\n",
      "      dtype='object', length=606199)\n"
     ]
    }
   ],
   "source": [
    "print(bdata.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 8221 × 606199\n",
      "    obs: 'cluster', 'n_genes'\n",
      "    obsm: 'X_umap'\n"
     ]
    }
   ],
   "source": [
    "print(bdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       4511\n",
      "1       1760\n",
      "2       1193\n",
      "3       2432\n",
      "4       1224\n",
      "        ... \n",
      "8216    1696\n",
      "8217    1992\n",
      "8218    1330\n",
      "8219    1161\n",
      "8220    1081\n",
      "Name: n_genes, Length: 8221, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(bdata.obs['n_genes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the preprocess.py\n",
    "bin file: This can be understood as a list of bins which in our model, the DNA sequence is splitted into 5000 sequence bins.\n",
    "\n",
    "common_bins is compring of bins which exist in both of the original atac data and the bin-file.\n",
    "\n",
    "If not intersect, then a new bdata is created with rows representing cells in adata while the columns are bins in the bin_file. Then we fill the common bins with the data in adata corresponding to it. The non-overlapping peaks will be remained as 0.\n",
    "\n",
    "If intersect, we pick submatrix from the original adata.X by taking the data in the common_bins.\n",
    "\n",
    "Filtering cells by counts: filter out cells whose number of open peaks will smaller then the setting value and finally an extra information about rows in bdata.X will be added, that is obs will contain an extra key for illustrating the number of open peaks in each cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Table\n",
    "创建这个class需要包含的信息： name of data table, data(DataSet)\n",
    "\n",
    "可以调用的函数：save(path, format) 将Data Table以指定形式储存在指定的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Information\n",
    "创建这个class需要包含on_disk_path, on_disk_format 和 main_table_key信息， \n",
    "\n",
    "manifests = {\"on_disk_format\": self.on_disk_format,\"main_data\": self.main_table_key, }\n",
    "\n",
    "main_table_key表示的是哪一个数据是重要的\n",
    "\n",
    "可以调用的函数：\n",
    "\n",
    "（1） save(path, suffix) /path/manifest.suffix.json中保存manifests\n",
    "\n",
    "（2） load(path) 通过path来load存在这个path上的manifests，并且将manifests里面所包含的信息放在self.on_disk_format 和 self.main_table_key中\n",
    "\n",
    "（3） from_path(path)创建一个新的MetoInfo对象，然后将这个对象的self.on_disk_path设置为输入函数的path。然后调用load(path)函数将path下的manifest.json文件中的内容加载到自己的属性中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "创建这个class所需要的包含的初始化信息有：remove_zero_rows(T/F), max_tokenize_batch_size(integer), immediate_save(T/F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBank\n",
    "创建一个DataBank class需要的初始化信息：meta_info（class MetaInf), data_tables(a dictionary; key:name of data_table, value: the corresponding data table of each key)\n",
    "DataBank 可以被理解为包含着多个data table还有data table所对应的meta_information。\n",
    "\n",
    " (1) __post_init__(): 这个函数在创建DataBank类的时候会自动call，在只有metaInfor的时候，用sync()来自动初始化信息。 用_validate_data() 验证data tables 和 main_table_key之间的匹配性。也就是检查main_table_key在不在data_tables的key中，或者一些其他的错误。\n",
    "\n",
    "\n",
    "\n",
    "（2） sync():用来同步data table / meta_information /both of them 会直接在data table和 meta_information储存的地方直接覆盖原本的信息。\n",
    "\n",
    "（3） main_data():得到data_tables中main_table_key所对应的表\n",
    "\n",
    "（4） _validate_data(): 验证data tables 和 main_table_key之间的匹配性。也就是检查main_table_key在不在data_tables的key中，或者一些其他的错误。\n",
    "\n",
    "（5）load_anndata（adata: AnnData,data_keys: Optional[List[str]] = None,token_col: str = \"gene name\"） \n",
    "token_col可以用来指定一个bin的名字，如果没有指定bin就会将所有的bin的名字来形成一个tokens（a list comprising of all bins' name or the name specified by token_col)\n",
    "随后得到tokens里面所有bin属于的染色体和初始碱基位置，然后对于data_keys中的每一个key调用 （6） _load_anndata_layer得到一个Dataset，然后将这些Dataset append 到data_tables which is a list comprising of data_table. Each data table belongs to DataTable class: name is the data name (\"X\", \"raw\") and the data is a dataset comprising of three columns (cell_id, non_zero_bin_chr, non_zero_bin_pos)\n",
    "\n",
    "问题：这个函数中的chr is a list of integers while for applying function defined in (6),it should be a list of str.\n",
    "\n",
    "（6） _load_anndata_layer(adata: AnnData, chr: List[str], pos: List[int],data_key: Optional[str] = \"X\",): \n",
    "data_key 可能take的值：“X” 或者是adata.layers 或 adata.obsm 中的key值。主要结构是调用7）_tokenize来得到包含\"id\",\"chr\",\"pos\" as illustrated in (7) 的dictionary，然后调用Dataset.from_dict将字典转换为dataset，每个column表示dictionary里面的一个key，一个row表示一个细胞。\n",
    "\n",
    "\n",
    "（7）_tokenize(self, data: Union[np.ndarray, csr_matrix], chr: List[str], pos: List[int],) -> Dict[str, List]:\n",
    " Args:\n",
    "            data (np.ndarray or spmatrix): Data to be tokenized.\n",
    "            chr: list of chromosome names for each bin.\n",
    "            pos: list of chromosome positions for each bin.\n",
    " Returns:\n",
    "         Dict[str, List]: Tokenized data.\n",
    "\n",
    "Tokenized data: a dictionary comprising of three values: \n",
    "\"id\": id of cells(the first cell will be denoted as 1), \n",
    "\"chr\": a list comprising of sublists which each sublist denotes the \"chr\" corresponding to the non-zero bin in the specific cell, \n",
    "\"pos\": a list comprising of sublists which each sublist denotes the initial position of the DNA sequence corresponding to the non-zero bin in the specific cell\n",
    "\n",
    "\n",
    "csr_matrix：的形式\n",
    "[[ 0,  5,  0,  0],\n",
    " [ 3,  0,  0,  2],\n",
    " [ 0,  0,  0,  4]]\n",
    "data = [5, 3, 2, 4] #所有非零\n",
    "indices = [1, 0, 3, 3] #每一个非零值在它所在的row上的index\n",
    "indptr = [0, 1, 3, 4] #每行（细胞）在 data/indices 中的起始和结束位置\n",
    "\n",
    "(8) update_datatables(self,new_tables: List[DataTable], #指定每个数据表的name use_names: List[str] = None, overwrite: bool = False, immediate_save: Optional[bool] = None,) -> None:\n",
    "        \"\"\"\n",
    "        Update the data tables in the DataBank with new data tables.\n",
    "\n",
    "        Args:\n",
    "            new_tables (list of :class:`DataTable`): New data tables to update.\n",
    "            use_names (list of :obj:`str`): Names of the new data tables to use.\n",
    "                If not provided, will use the names of the new data tables.\n",
    "            overwrite (:obj:`bool`): Whether to overwrite the existing data tables.\n",
    "            immediate_save (:obj:`bool`): Whether to save the data immediately after\n",
    "                updating. Will save to :attr:`self.meta_info.on_disk_path`. If not\n",
    "                provided, will follow :attr:`self.settings.immediate_save` instead.\n",
    "                Default to None.\n",
    "        \"\"\"\n",
    "    将新的data_table以正确的方式加入原本的data_tables里面或将原本data_tables里面的data_table改写（直接覆盖）\n",
    "（9）from_path(cls, path: Union[Path, str]) -> Self 将提供路径中的datatable文件转位DataTable类后全部写入一个新的DataBank类里面，新的DataBank类包含原路径中的Meta_Inf\n",
    "\n",
    "（10）_nparray2indexed_values(data: np.ndarray （cell-by-bin matrix), chr: List[str], pos: List[int],) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "    Convert a numpy array to indexed values. Only include the non-zero values.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Data matrix.\n",
    "        chr: list of chromosome names for each bin.\n",
    "        pos: list of chromosome positions for each bin.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List, List, List]: Row IDs, chromosome names, and chromosome positions.\n",
    "    \"\"\"\n",
    "    当数据不是crs_matrix的时候，要怎么得到三个list：\n",
    "a list comprising of id of cells(the first cell will be denoted as 1), \n",
    "a list comprising of sublists which each sublist denotes the \"chr\" corresponding to the non-zero bin in the specific cell, \n",
    "a list comprising of sublists which each sublist denotes the initial position of the DNA sequence corresponding to the non-zero bin in the specific cell\n",
    "\n",
    "（11）_nparray2indexed_values_numba() 作用和（10）是完全一样的，只是速度会更快\n",
    "\n",
    "（12）_nparray2mapped_values(）作用和(10),(11)完全一样，只是选择其中的一种来用\n",
    "\n",
    "(13) from_anndata(cls, adata: Union[AnnData, Path, str], to: Union[Path, str], main_table_key: str = \"X\", token_col: Optional[str] = None, immediate_save: bool = True,) -> Self:\n",
    "        \"\"\"\n",
    "        Create a DataBank from an AnnData object.\n",
    "\n",
    "        Args:\n",
    "            adata (AnnData): Annotated data or path to anndata file.\n",
    "            to (Path or str): Data directory.\n",
    "            main_table_key (str): This layer/obsm in anndata will be used as the\n",
    "                main data table.\n",
    "            token_col (str): Column name of the gene token.\n",
    "            immediate_save (bool): Whether to save the data immediately after creation.\n",
    "        Returns:\n",
    "            DataBank: DataBank instance.\n",
    "        \"\"\"\n",
    "    加载提供路径的anndata文件后，在to的路径下创建一个DataBank，将DataBank的main_table_key按照输入值设定，然后call函数（5）load_anndata（adata: AnnData,data_keys: Optional[List[str]] = None,token_col: str = \"gene name\"）得到 data_tables which is a list comprising of data_table. Each data table belongs to DataTable class: name is the data name (\"X\", \"raw\") and the data is a dataset comprising of three columns (cell_id, non_zero_bin_chr, non_zero_bin_pos)然后选取第一个，也就是“X”所对应的dataset。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['build_large_scale_data.py',\n",
    "            '--input-dir', '/lustre/project/Stat/1155223034/atacFormer/data/heart', \n",
    "            '--bin-file', '/lustre/project/Stat/1155223034/atacFormer/data/bins_5k_table_23chr.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 files in /lustre/project/Stat/1155223034/atacFormer/data/heart\n",
      "\n",
      "Processing HBM394.PKDH.674_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.PKDH', '.674_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.674_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.PKDH', '.674_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.674_ATAC', '.h5ad'].\n",
      "originally read (2764, 533212) data from HBM394.PKDH.674_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 528446\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (2763, 528446) valid data from HBM394.PKDH.674_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM394.PKDH.674.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  4.21ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HBM656.KSMZ.578_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.KSMZ', '.578_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.578_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.KSMZ', '.578_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.578_ATAC', '.h5ad'].\n",
      "originally read (16279, 534461) data from HBM656.KSMZ.578_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 531350\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (15647, 531350) valid data from HBM656.KSMZ.578_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM656.KSMZ.578.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 16/16 [00:01<00:00, 15.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HBM472.QPXD.658_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.QPXD', '.658_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.658_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.QPXD', '.658_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.658_ATAC', '.h5ad'].\n",
      "originally read (32828, 537709) data from HBM472.QPXD.658_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 533441\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (32264, 533441) valid data from HBM472.QPXD.658_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM472.QPXD.658.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 33/33 [00:02<00:00, 12.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HBM983.ZLKS.328_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.ZLKS', '.328_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.328_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.ZLKS', '.328_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.328_ATAC', '.h5ad'].\n",
      "originally read (2438, 535288) data from HBM983.ZLKS.328_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 530600\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (2436, 530600) valid data from HBM983.ZLKS.328_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM983.ZLKS.328.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 14.68ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HBM233.GKRM.627_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.GKRM', '.627_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.627_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.GKRM', '.627_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.627_ATAC', '.h5ad'].\n",
      "originally read (24982, 536282) data from HBM233.GKRM.627_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 530171\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (21949, 530171) valid data from HBM233.GKRM.627_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM233.GKRM.627.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 22/22 [00:00<00:00, 23.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HBM477.BPMF.855_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.BPMF', '.855_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.855_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.BPMF', '.855_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.855_ATAC', '.h5ad'].\n",
      "originally read (21016, 536104) data from HBM477.BPMF.855_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 530043\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (18714, 530043) valid data from HBM477.BPMF.855_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM477.BPMF.855.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:00<00:00, 22.67ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing HBM772.PDCW.864_ATAC.h5ad\n",
      "WARNING: Your filename has more than two extensions: ['.PDCW', '.864_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.864_ATAC', '.h5ad'].\n",
      "WARNING: Your filename has more than two extensions: ['.PDCW', '.864_ATAC', '.h5ad'].\n",
      "Only considering the two last: ['.864_ATAC', '.h5ad'].\n",
      "originally read (1148, 518559) data from HBM772.PDCW.864_ATAC.h5ad\n",
      "Filtering bins ...\n",
      "606199\n",
      "Common bins: 514897\n",
      "Filtering cells by counts ...\n",
      "Binarizing data ...\n",
      "read (1138, 514897) valid data from HBM772.PDCW.864_ATAC.h5ad\n",
      "DataBank - INFO - No suffix provided, saving to default.\n",
      "DataBank - INFO - Saving data table X to /lustre/project/Stat/1155223034/atacFormer/data/heart/HBM772.PDCW.864.scb/datatable.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 25.82ba/s]\n"
     ]
    }
   ],
   "source": [
    "import build_large_scale_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atacformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
