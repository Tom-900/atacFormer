def train(model: nn.Module, 
          train_loader: DataLoader,
          epoch: int) -> None:
    """
    Train the model for one epoch.
    """
    model.train()
    # total loss: mlm loss + cls loss
    # total acc: 0, 1, 2 accuracy
    # mlm acc: accuracy for 2
    # or: organ; ct: celltype
    total_loss, total_mlm_loss, total_cls_loss, total_acc, total_mlm_acc,\
        total_og_acc, total_ct_acc = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
    
    log_interval = args.log_interval
    start_time = time.time()

    num_batches = len(train_loader)
    for batch, data_dict in enumerate(train_loader):
        global_iter = epoch * num_batches + batch

        # load the data on CPU
        input_chr = data_dict["masked_chr"] # (batch_size, seq_len)
        input_pos = data_dict["masked_pos"]
        
        # load DNA embedding on CPU
        if args.use_memmap:
            # the index of the bin for extracting the DNA-seq
            bin_ids_seq = chr_pos_to_idx(input_chr, input_pos, num_bins_list, special_to_zero=True) # (batch_size, seq_len)
            bin_ids_seq = bin_ids_seq.view(-1).numpy() # (batch_size * seq_len)
            dna_emb = torch.tensor(dna_emb_table[bin_ids_seq]) # (batch_size * seq_len, dna_emb_dim)
        else:
            bin_ids_seq = chr_pos_to_idx(input_chr, input_pos, num_bins_list, special_to_zero=True) # (batch_size, seq_len)
            bin_ids_seq = bin_ids_seq.view(-1)
            dna_emb = dna_emb_table[bin_ids_seq]
            
        dna_emb = dna_emb.view(input_chr.size(0), input_chr.size(1), -1)
        dna_emb = dna_emb.to(device)
        
        # mask
        src_key_padding_mask = input_chr.eq(args.pad_value) # (batch_size, seq_len)
        
        # to device
        data_dict = {k: v.to(device) for k, v in data_dict.items()}
        dna_emb = dna_emb.to(device)
        src_key_padding_mask = src_key_padding_mask.to(device)
        
        with torch.cuda.amp.autocast(enabled=args.fp16):
            output_dict = model(
                    seq=dna_emb,
                    data_dict=data_dict,
                    src_key_padding_mask=src_key_padding_mask,
                    use_cls=USE_CLS,
                    )
            
            predictions = output_dict["predictions"] # list[(batch, num_bins[i], 3) for i in range(23)]
            formulated_targets = output_dict["formulated_targets"] # list[(batch, num_bins[i]) for i in range(23)]
            
            mlm_loss = []
            for c in range(23):
                 mlm_loss_c = criterion(predictions[c].view(-1, 3), formulated_targets[c].view(-1))
                 mlm_loss.append(mlm_loss_c)
            
            mlm_loss = torch.stack(mlm_loss).mean()
            writer.add_scalar("train/mlm", mlm_loss, global_iter)
            loss = mlm_loss.clone()
            
            if USE_CLS:
                og_labels = data_dict["organ"]
                ct_labels = data_dict["celltypes"]
                og_loss = criterion_cls(output_dict["og_logits"], og_labels)
                ct_loss = criterion_cls(output_dict["ct_logits"], ct_labels)
                cls_loss = og_loss + ct_loss
                loss = loss + args.weight_cls * cls_loss
                writer.add_scalar("train/cls", cls_loss, global_iter)

            writer.add_scalar("train/loss", loss, global_iter)

        if args.grad_accu_steps > 1:
            loss = loss / args.grad_accu_steps
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        if args.grad_accu_steps > 1:
            if batch % args.grad_accu_steps == 0 or batch == num_batches - 1:
                scheduler.step()
                optimizer.zero_grad()
        else:
            scheduler.step()
            optimizer.zero_grad()
            
        # calculate the accuracy
        with torch.no_grad():
            acc, mlm_acc, mlm_num = 0.0, 0.0, 0.0
            for c in range(23):
                acc += (predictions[c].argmax(dim=-1) == formulated_targets[c]).float().sum()
                mlm_mask = formulated_targets[c] == args.mask_value
                if mlm_mask.sum() > 0:
                    mlm_num += mlm_mask.sum()
                    mlm_acc += ((predictions[c].argmax(dim=-1) == formulated_targets[c]) * mlm_mask).float().sum()
                
            acc = acc / sum(num_bins_list)
            mlm_acc = mlm_acc / mlm_num if mlm_num > 0 else 0.0
            writer.add_scalar("train/acc", acc, global_iter)
            writer.add_scalar("train/mlm_acc", mlm_acc, global_iter)
            
            if USE_CLS:
                og_acc = (output_dict["og_logits"].argmax(dim=-1) == og_labels).float().mean()
                ct_acc = (output_dict["ct_logits"].argmax(dim=-1) == ct_labels).float().mean()
                writer.add_scalar("train/og_acc", og_acc, global_iter)
                writer.add_scalar("train/ct_acc", ct_acc, global_iter)

        total_loss += loss.item()
        total_mlm_loss += mlm_loss.item()
        total_acc += acc.item()
        total_mlm_acc += mlm_acc.item()
        total_cls_loss += cls_loss.item() if USE_CLS else 0.0
        total_og_acc += og_acc.item() if USE_CLS else 0.0
        total_ct_acc += ct_acc.item() if USE_CLS else 0.0
        
        if args.local_rank in [0, -1] and batch % log_interval == 0 and batch > 0:
            # Writer logs gradients distribution
            for name, param in model.named_parameters():
                if param.requires_grad and param.grad is not None:
                    writer.add_histogram(name + "_grad", param.grad, global_iter)
                    writer.add_histogram(name + "_param", param, global_iter)

            # Log scalar values
            lr = scheduler.get_last_lr()[0]
            ms_per_batch = (time.time() - start_time) * 1000 / log_interval
            cur_loss = total_loss / log_interval
            cur_mlm_loss = total_mlm_loss / log_interval
            cur_acc = total_acc / log_interval
            cur_cls_loss = total_cls_loss / log_interval if USE_CLS else 0.0
            cur_og_acc = total_og_acc / log_interval if USE_CLS else 0.0
            cur_ct_acc = total_ct_acc / log_interval if USE_CLS else 0.0
            
            # ppl = math.exp(cur_loss)
            logger.info(
                f"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | "
                f"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | "
                f"loss {cur_loss:5.2f} | mlm_loss {cur_mlm_loss:5.2f} |"
                + (f"acc {cur_acc:5.2f} | ")
                + (f"cls {cur_cls_loss:5.2f} | " if USE_CLS else "")
                + (f"og_acc {cur_og_acc:5.2f} | " if USE_CLS else "")
                + (f"ct_acc {cur_ct_acc:5.2f} | " if USE_CLS else "")
            )
            writer.add_scalar("lr", lr, global_iter)

            total_loss, total_mlm_loss, total_cls_loss, total_acc, total_mlm_acc, total_og_acc, \
                total_ct_acc = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
            start_time = time.time()

        # immediately eval and save
        if batch % args.save_interval == 0 and batch > 0:
            eval_and_save(model, valid_loader, global_iter)
            model.train()  # important, reset to train mode

